\section{Theoretische Grundlagen}

\subsection{Multimodale Daten}
\label{sec:multimodal_data}

Eine \textit{Modalität} bezeichnet eine eigenständige Form der Informationsrepräsentation mit spezifischer statistischer Struktur und Verarbeitungslogik (z. B. sequenzieller Text vs. tabellarische Attribute). Multimodales Lernen nutzt diese unterschiedlichen und sich ergänzenden Perspektiven auf denselben Sachverhalt, erfordert jedoch spezialisierte Strategien zur gemeinsamen Modellierung \cite{Baltrusaitis2019}.

\subsubsection{Heterogenität in multimodalen Daten}
\label{sec:heterogeneity}

Die Kombination von Text- und Tabellendaten stellt hierbei eine besondere Herausforderung dar. Während Text linguistisch strukturiert ist und Semantik aus der Reihenfolge von Tokens gewinnt, sind Tabellen attributbasiert und nicht-sequenziell. Diese strukturelle Diskrepanz führt zu einem „Manifold Mismatch“ \cite{Baltrusaitis2019, Huang2020}: Text wird in hochdimensionalen, kontinuierlichen Embedding-Räumen repräsentiert, während Tabulardaten aus heterogenen, unskalierten Merkmalen bestehen. Die geometrische Inkompatibilität dieser Räume verhindert, dass Attention-Mechanismen, die auf Text-Tokens effektiv angewendet werden können, direkt auf rohe Tabularfeatures funktionieren.

Die zentrale Herausforderung besteht somit darin, diese strukturelle Lücke zu überbrücken. Bevor eine Fusion technisch möglich ist, müssen beide Modalitäten in einen kompatiblen, gemeinsamen Vektorraum projiziert werden. Dies erfordert spezifische Repräsentationsstrategien: Während für Text etablierte Tokenisierungsverfahren existieren, müssen für tabellarische Daten erst analoge Methoden der Tokenization angewendet werden, um sie in eine verarbeitbare Sequenzform zu überführen. Die folgenden Abschnitte erläutern die Umsetzung dieser Repräsentationen für Text (Abschnitt \ref{sec:text_representation}) und Tabellen (Abschnitt \ref{sec:tab_representation}) im Detail.

\subsubsection{Repräsentation von Textdaten}
\label{sec:text_representation}

Für die effektive Verarbeitung natürlicher Sprache (\Gls{nlp}) und die in dieser Arbeit untersuchte Fusion von Text- und Tabellendaten ist die Transformation von Informationen in numerische Vektorrepräsentationen unerlässlich \cite{Abubakar2022}. Um das Verständnis der nachfolgenden Architektur-Entscheidungen zu erleichtern, werden zunächst die grundlegenden Konzepte der Tokenisierung und Repräsentation definiert:

\begin{itemize}
    \item \textbf{Token}: Ein Token stellt die kleinste diskrete Einheit einer Eingabesequenz dar, in die Rohdaten zerlegt werden, bevor sie ein Modell verarbeitet. Während dies im Textbereich Wörter oder Wortteile (Sub-words) sein können \cite{Devlin2019}, werden in multimodalen Modellen auch Bild-Patches \cite{Dosovitskiy2021} als Tokens behandelt. Im Modell dienen Tokens als eindeutige Identifikatoren (Token IDs), die eine Verbindung zwischen Rohdaten und ihrer mathematischen Repräsentation (Vektoren) herstellen.
    \item \textbf{Repräsentation}: Die Repräsentation ist die numerische Kodierung (Vektorisierung) von Informationen, die es einem Modell erlaubt, die Semantik, Struktur oder Merkmale von Daten mathematisch zu erfassen \cite{Abubakar2022}.
    \item \textbf{Embedding}: Ein Embedding ist eine gelernte, dichte Vektorrepräsentation. Man unterscheidet hierbei zwischen statischen Einbettungen (z. B. Word2Vec \cite{Mikolov2013}), die semantische Grundbeziehungen abbilden, und \textit{kontextualisierten Repräsentationen} (z. B. \gls{bert} \cite{Devlin2019}), bei denen die mathematische Bedeutung eines Elements dynamisch durch seine Beziehung zu umgebenden Datenpunkten bestimmt wird.
\end{itemize}

Zur Veranschaulichung dient folgende Analogie: Wenn Daten eine Sprache wären, dann entsprechen die \textit{Tokens} den einzelnen Buchstaben oder Wörtern in einem Wörterbuch, während die \textit{Repräsentation} die tiefere Bedeutung ist, die diese Wörter erst in einem konkreten Satz ergeben.

Historisch betrachtet basierten frühe Ansätze auf frequenzbasierten Verfahren wie dem Bag-of-Words-Modell oder der \Gls{tfidf} \cite{Salton1988}. Diese Methoden repräsentieren Dokumente als Vektoren von Worthäufigkeiten, ignorieren jedoch weitgehend die grammatikalische Struktur und die semantische Bedeutung der Wörter. Zudem führen sie oft zu hochdimensionalen, dünn besetzten Vektoren (sparse vectors).

Einen signifikanten Fortschritt markierte die Einführung von verteilten Wortrepräsentationen (Word Embeddings), insbesondere durch das Word2Vec-Verfahren \cite{Mikolov2013}. Hierbei werden Wörter in einen dichten, niedrigdimensionalen Vektorraum projiziert, wobei diese Projektionen durch einfache neuronale Netze gelernt werden. Semantisch ähnliche Wörter liegen in diesem Vektorraum nahe beieinander. Ein wesentlicher Nachteil dieser statischen Embeddings besteht jedoch darin, dass jedem Wort unabhängig von seinem Kontext ein fixer Vektor zugewiesen wird. Polyseme Wörter, die je nach Satzkontext unterschiedliche Bedeutungen haben, können so nicht adäquat abgebildet werden.

Um dieses Defizit zu beheben, wurden kontextuelle Embeddings entwickelt. Peters et al. stellten mit \Gls{elmo} einen Ansatz vor, der tiefere neuronale Netze nutzt, um kontextabhängige Wortvektoren zu generieren \cite{Peters2018}. Den aktuellen Standard setzen jedoch Transformer-basierte Modelle wie \Gls{bert}, eingeführt von Devlin et al. \cite{Devlin2019}. Als Encoder-only-Modell wird \gls{bert} auf großen Textmengen vor-trainiert und erzeugt durch ein bidirektionales Training tiefgreifende kontextuelle Repräsentationen. Im Gegensatz zu \gls{elmo} basiert \gls{bert} auf der Transformer-Architektur und nutzt intensiv Self-Attention-Mechanismen, deren technische Details in Abschnitt \ref{sec:transformer_attention} erläutert werden.

Für die Verarbeitung ganzer Sätze oder Dokumente, wie sie in dieser Arbeit relevant ist, stoßen reine Token-Embeddings jedoch an Grenzen. Hier kommen Techniken wie Pooling oder die Verwendung spezieller Tokens ins Spiel. Insbesondere das \texttt{[CLS]}-Token (Classifier Token) in Modellen wie \gls{bert} wird häufig genutzt, um eine aggregierte Repräsentation des gesamten Satzes zu erhalten. Dieses spezielle Token wird jeder Eingabesequenz vorangestellt und sammelt während der Verarbeitung durch die Transformer-Schichten über Self-Attention kontextuelle Informationen des gesamten Inputs. Der finale Vektor des \texttt{[CLS]}-Tokens ($T_{[CLS]}$) dient somit als kompakte Repräsentation der gesamten Eingabe und wird in Klassifikationsaufgaben üblicherweise verwendet, um die Vorhersage zu treffen \cite{Devlin2019}. Die Nutzung des \texttt{[CLS]}-Tokens ermöglicht stabile semantische Kodierungen auf Satzebene, welche die notwendige Voraussetzung schaffen, um Textinformationen effizient mit anderen Modalitäten, wie tabellarischen Merkmalen, in einer multimodalen Architektur zu fusionieren.

\subsubsection{Repräsentation tabellarischer Daten}
\label{sec:tab_representation}

Tabellarische Daten bilden die Grundlage zahlreicher Anwendungen in der Industrie, stellen jedoch für Deep-Learning-Ansätze eine besondere Herausforderung dar \cite{Borisov2024}. Um die in Abschnitt \ref{sec:heterogeneity} beschriebene Lücke zu schließen, ist eine Transformation der rohen Tabellendaten in eine kompatible Vektor-Repräsentation notwendig.

\paragraph{Herausforderungen der Tabellenstruktur}
Tabellen zeichnen sich durch eine starke Heterogenität aus. Sie bestehen aus einem Mix von dichten numerischen Features (kontinuierliche Werte wie Preis oder Alter) und sparsen kategorialen Features (diskrete Werte wie Postleitzahl oder Produktkategorie) \cite{Borisov2024, Badaro2023}. 

Ein weiteres wesentliches Merkmal ist die Permutationsinvarianz der Spalten. Anders als Wörter in einem Satz haben die Spalten einer Tabelle keine natürliche Ordnung. Ein Modell muss daher robust gegenüber der Vertauschung von Feature-Positionen sein, solange die Zuordnung von Wert und Feature-Typ erhalten bleibt \cite{Gorishniy2021}. Die Repräsentation $(Feature_A, Feature_B)$ muss für das Netzwerk semantisch identisch zu $(Feature_B, Feature_A)$ verarbeitet werden.

\paragraph{Grenzen klassischer Vorverarbeitungsmethoden}
In traditionellen Machine-Learning-Verfahren, wie Gradient Boosted Decision Trees (GBDT), werden kategoriale Daten häufig mittels One-Hot-Encoding (OHE) verarbeitet. Dabei wird eine kategoriale Variable mit der Kardinalität $C$ in einen binären Vektor der Länge $C$ transformiert. Für Deep-Learning-Modelle und insbesondere für die Integration in Transformer-Architekturen weist dieses Verfahren jedoch gravierende Nachteile auf:

Zum einen führt OHE bei Features mit hoher Kardinalität zu extrem hochdimensionalen und dünn besetzten Vektoren (Sparsity). Dies erschwert es neuronalen Netzen, dichte und aussagekräftige Repräsentationen zu lernen, da der Großteil der Eingabewerte Null ist \cite{Huang2020, Gorishniy2021}. 
Zum anderen fehlt OHE jegliche semantische Information. Alle Kategorien werden als orthogonal betrachtet, was bedeutet, dass der Abstand zwischen allen Kategorien im Vektorraum identisch ist. Semantische Beziehungen, wie etwa die Ähnlichkeit zwischen den Kategorien „Auto“ und „LKW“ im Vergleich zu „Apfel“, können durch OHE nicht abgebildet werden \cite{Huang2020}. 

Auch einfache Multi-Layer Perceptrons (MLPs) stoßen auf rohen Daten an ihre Grenzen, da sie numerische Eingaben lediglich als skalare Werte betrachten und im ersten Schritt keine feature-spezifische Verarbeitung ermöglichen, wie sie beispielsweise für das Erlernen von Interaktionen zwischen spezifischen Features notwendig wäre \cite{Gorishniy2021}.

\paragraph{Tabular Tokenization und Latenter Raum}
Der Prozess, tabellarische Daten in eine für Transformer verarbeitbare Sequenz zu überführen, wird allgemein als \textit{Tabular Tokenization} bezeichnet. Ziel ist die Projektion der heterogenen Rohdaten in einen gemeinsamen latenten Raum \cite{Borisov2024, Badaro2023}. 

Das Ziel ist es, eine Tabellenzeile $x$ in eine Sequenz von Vektoren $\mathbf{E}$ zu transformieren, die strukturell der Eingabe eines Sprachmodells gleicht:
\begin{equation}
    \mathbf{E} = [\mathbf{e}_1, \mathbf{e}_2, \dots, \mathbf{e}_N] \quad \text{mit} \quad \mathbf{e}_i \in \mathbb{R}^H
\end{equation}
Hierbei entspricht $H$ der Dimension des Hidden-States des verwendeten Transformer-Modells (z. B. $H=768$ bei \gls{bert}). Durch diese Angleichung der Dimensionen wird die technische Voraussetzung geschaffen, um später Mechanismen wie Cross-Attention zwischen Text- und Tabellen-Tokens anzuwenden \cite{Badaro2023}. Im Gegensatz zu OHE erlaubt dieser dichte Vektorraum das Erlernen semantischer Ähnlichkeiten, sodass verwandte Features im Vektorraum geometrisch näher beieinander liegen.

Es ist jedoch essenziell zu unterscheiden: Während bei Text-Tokens die Reihenfolge die semantische Bedeutung bestimmt, handelt es sich bei \textit{Tabular Tokens} um eine künstlich erzeugte Sequenz ohne natürliche Ordnung. Die Sequenzform dient hier rein als technisches Konstrukt für die Attention-Verarbeitung, nicht als Träger syntaktischer Informationen.


\paragraph{Architekturen der Repräsentation}
Hinsichtlich der Struktur der erzeugten Sequenz lassen sich zwei Hauptstrategien unterscheiden, die den Unterschied zwischen spaltenweiser und zeilenweiser Verarbeitung verdeutlichen \cite{Badaro2023}:

Der erste Ansatz, bekannt als \textbf{Feature Tokenization} (oder Column-wise Embeddings), bildet jedes Feature der Ursprungstabelle isoliert auf genau einen Token-Vektor ab. Dies wird beispielsweise im \textit{TabTransformer} \cite{Huang2020} oder im \textit{SAINT}-Modell \cite{Somepalli2021} angewendet. Die Sequenzlänge $N$ entspricht hierbei exakt der Anzahl der Spalten. Der Vorteil liegt in der Erhaltung der feingranularen Information jedes einzelnen Features.

Der zweite Ansatz, der in dieser Arbeit auch Anwendung findet, verfolgt eine \textbf{globale Projektion} (Row-level Tokenization). Hierbei werden alle Features einer Zeile zunächst konkateniert und gemeinsam durch ein MLP geleitet, um eine Sequenz von latenten Tokens zu erzeugen, die nicht mehr zwangsläufig 1-zu-1 den ursprünglichen Spalten entsprechen \cite{Gu2021}. Dies ermöglicht eine Entkopplung der Sequenzlänge von der Anzahl der Eingabefeatures und kann die Rechenkomplexität in der nachfolgenden Attention-Schicht reduzieren. Entscheidend ist hierbei, dass das Modell bereits in der Tokenization-Phase globale Zusammenhänge zwischen den Features aggregieren kann, anstatt sie isoliert zu betrachten.

Unabhängig von der gewählten Methode ist das Ergebnis dieser Transformation eine Sequenz dichter Vektoren. Erst durch diesen Schritt wird die „Sprache“ der Tabelle in die „Sprache“ des Transformers übersetzt, was die Basis für die in dieser Arbeit untersuchte multimodale Fusion mittels Cross-Attention bildet.

\subsection{Transformer-Architekturen und Attention-Mechanismen}
\label{sec:transformer_attention}

Nachdem im vorangegangenen Kapitel die Grundlagen der Datenrepräsentation für Text und Tabellen erläutert wurden, widmet sich dieser Abschnitt der Modellarchitektur, die den aktuellen Stand der Technik in der Verarbeitung natürlicher Sprache definiert: dem Transformer. Ursprünglich für maschinelle Übersetzungsaufgaben konzipiert, hat sich diese Architektur als leistungsfähiger Standard für die Modellierung komplexer Abhängigkeiten in sequenziellen Daten etabliert. Das Verständnis der internen Mechanismen, insbesondere der Attention, ist eine notwendige Voraussetzung für die in dieser Arbeit entwickelte multimodale Fusion.

\subsubsection{Der Transformer-Encoder}

Das Transformer-Modell wurde 2017 von Vaswani et al. eingeführt und stellt einen signifikanten Fortschritt im Deep Learning dar, da es auf Rekurrenz und Faltung verzichtet und stattdessen primär auf Attention-Mechanismen basiert \cite{Vaswani2017}. Die ursprüngliche Architektur besteht aus zwei Hauptkomponenten: einem Encoder, der die Eingabesequenz verarbeitet und in eine abstrakte Repräsentation überführt, und einem Decoder, der basierend darauf eine Ausgabesequenz generiert.

Für die Aufgabenstellung dieser Arbeit, bei der es um die Klassifikation von multimodalen Daten geht, ist primär der Encoder-Teil von Relevanz. Encoder-only-Modelle, wie das in Abschnitt \ref{sec:text_representation} vorgestellte \gls{bert}, nutzen einen Stapel von Transformer-Blöcken, um für jedes Eingabe-Token einen kontextualisierten Vektor zu berechnen \cite{Devlin2019}. Im Gegensatz zu statischen Embeddings, bei denen ein Wort stets denselben Vektor erhält, integriert der Transformer-Encoder Informationen aus dem gesamten Kontext der Sequenz in die Repräsentation jedes einzelnen Tokens. Der Encoder fungiert somit als Feature-Extraktor, der in der Lage ist, syntaktische und semantische Beziehungen innerhalb der Daten abzubilden.

\subsubsection{Scaled Dot-Product Attention}

Das zentrale Element eines jeden Transformer-Blocks ist der Attention-Mechanismus. Er ermöglicht es dem Modell, die Relevanz verschiedener Teile der Eingabe für den aktuellen Verarbeitungsschritt dynamisch zu gewichten. Vaswani et al. formalisieren dies als \textit{Scaled Dot-Product Attention} \cite{Vaswani2017}.

Das Konzept lässt sich abstrakt als ein Abfrage-Prozess beschreiben: Eine Abfrage (\textit{Query} $Q$) wird mit einer Menge von Schlüsseln (\textit{Keys} $K$) verglichen, um die korrespondierenden Werte (\textit{Values} $V$) zu aggregieren. Mathematisch wird dies durch folgende Gleichung ausgedrückt:

\begin{equation}
    \text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V
\end{equation}

Hierbei berechnet das Skalarprodukt $QK^T$ die Ähnlichkeit zwischen der Query und den Keys. Der Faktor $\frac{1}{\sqrt{d_k}}$ dient der Skalierung, um numerische Instabilitäten bei großen Dimensionen zu vermeiden \cite{Vaswani2017}. Die Softmax-Funktion normalisiert diese Ähnlichkeitswerte zu Wahrscheinlichkeiten, die sich zu 1 aufsummieren. Schließlich wird eine gewichtete Summe der Values $V$ gebildet. Dies bedeutet, dass der resultierende Vektor primär Informationen von denjenigen Elementen enthält, die eine hohe Ähnlichkeit zur Query aufweisen.

\subsubsection{Self-Attention vs. Cross-Attention}

Ein wesentlicher Aspekt moderner Transformer-Architekturen und insbesondere multimodaler Modelle ist die Definition der Quellen für die Vektoren $Q$, $K$ und $V$. Hierbei wird zwischen \textit{Self-Attention} und \textit{Cross-Attention} unterschieden.

\paragraph{Self-Attention (Intra-Modal)}
Im Standard-Encoder, wie er von Vaswani et al. \cite{Vaswani2017} beschrieben wird, stammen Query, Key und Value aus derselben Eingabequelle (z.\,B. der Ausgabe des vorherigen Layers). Dies wird als Self-Attention bezeichnet.
\begin{itemize}
    \item \textbf{Funktionsweise:} Jedes Token der Sequenz berechnet seine Aufmerksamkeit in Bezug auf alle anderen Tokens derselben Sequenz.
    \item \textbf{Zweck:} Modellierung von Abhängigkeiten innerhalb einer Modalität. Dies erlaubt dem Modell, kontextuelle Beziehungen, wie etwa Referenzen innerhalb eines Satzes, zu erfassen.
\end{itemize}

\paragraph{Cross-Attention (Inter-Modal)}
Für die Fusion unterschiedlicher Modalitäten, wie Text und Tabellendaten, ist die Cross-Attention von zentraler Bedeutung. Dieses Konzept findet sich in diversen multimodalen Architekturen \cite{Tan2019}.
\begin{itemize}
    \item \textbf{Funktionsweise:} Die Vektoren werden aus unterschiedlichen Quellen gespeist. Beispielsweise generiert Modalität A (z.\,B. Text) die Queries $Q$, während Modalität B (z.\,B. Tabelle) die Keys $K$ und Values $V$ bereitstellt.
    \item \textbf{Zweck:} Ermöglichung eines Informationsflusses über Modalitätsgrenzen hinweg. Die Text-Repräsentationen können gezielt Informationen aus den Tabellen-Features integrieren, die für den aktuellen Kontext relevant sind. Dies bildet das theoretische Fundament für flexible Fusions-Architekturen, da es eine dynamische Verknüpfung heterogener Datenräume erlaubt.
\end{itemize}

\newpage
\subsection{Strategien der multimodalen Fusion}

Voraussetzung für jede Fusion ist, dass die unterschiedlichen Modalitäten bereits in einen kompatiblen Vektorraum transformiert wurden (vgl. Abschnitt \ref{sec:multimodal_data}). Die Strategien der multimodalen Fusion befassen sich anschließend mit der Frage, \textit{wie} (Mechanismen) und \textit{wann} (Architekturen) diese Informationen technisch kombiniert werden \cite{Li2024, Baltrusaitis2019}.

\subsubsection{Mechanismen der Fusion}

Die technische Realisierung der Fusion, also \textit{wie} die Vektoren der unterschiedlichen Modalitäten zusammengeführt werden, kann durch verschiedene mathematische Operationen erfolgen \cite{Jiao2024, Baltrusaitis2019}.
Ein zentrales Konzept ist hierbei die \textit{Joint Representation}. Das Ziel ist es, die unimodalen Eingangsvektoren in einen gemeinsamen semantischen Unterraum zu projizieren, um eine einzige, multimodale Repräsentation $Z$ zu erzeugen \cite{Jiao2024, Li2024}.
Mathematisch lässt sich dieser Vorgang allgemein formulieren als:
\begin{equation}
    Z = f(X_{\text{text}}, X_{\text{tab}})
\end{equation}
Wobei $f$ eine Funktion darstellt (beispielsweise ein neuronales Netz oder eine deterministische Operation), die aus den unimodalen Repräsentationen $X_{\text{text}}$ und $X_{\text{tab}}$ die fusionierte multimodale Repräsentation $Z$ berechnet \cite{Baltrusaitis2019}.

Zu den gängigsten spezifischen Operationen zählen:

\paragraph{Concatenation (Verkettung)}
Dies ist die wohl einfachste Form der Fusion, bei der die Merkmalsvektoren lediglich hintereinander gehängt werden:
\begin{equation}
    Z = [X_{\text{text}} \parallel X_{\text{tab}}]
\end{equation}
Ein wesentlicher Vorteil ist, dass die Eingangsvektoren $X_{\text{text}}$ und $X_{\text{tab}}$ nicht dieselbe Dimension besitzen müssen. Die Dimension des Ergebnisvektors $Z$ entspricht der Summe der Einzeldimensionen. In der Literatur wird dieser Ansatz oft als „Early Fusion“ bezeichnet, wenn er direkt auf der Ebene der Eingangsmerkmale angewendet wird \cite{Baltrusaitis2019}.

\paragraph{Addition (Summierung)}
Hierbei werden die Vektoren elementweise addiert:
\begin{equation}
    Z = X_{\text{text}} + X_{\text{tab}}
\end{equation}
Dies setzt zwingend voraus, dass beide Vektoren dieselbe Dimension aufweisen, weshalb oft eine vorherige Projektion notwendig ist. Diese Methode ist besonders in Joint-Representation-Ansätzen verbreitet \cite{Jiao2024}.

\paragraph{Attention-Mechanismen}
Im Gegensatz zu statischen Operationen wie der Addition oder Verkettung erlauben Attention-Mechanismen eine dynamische Gewichtung der Features.
Besonders relevant ist die \textit{Cross-Attention}. Hierbei können Repräsentationen einer Zielmodalität (z.\,B. Text) gezielt Informationen aus einer Quellmodalität (z.\,B. Tabelle) abfragen. Typischerweise stammen dabei die \textit{Queries} aus der Zielmodalität, während \textit{Keys} und \textit{Values} aus der Quellmodalität geliefert werden. Dies ermöglicht eine gewichtete Aggregation von Informationen, die auf den aktuellen Kontext konditioniert ist, und erlaubt die Modellierung komplexer Abhängigkeiten über Modalitätsgrenzen hinweg \cite{Li2024}.

\subsubsection{Architekturen und Positionierung der Fusion}

Neben der konkreten Fusionsoperation ist entscheidend, \textit{an welcher Stelle} im Modellfluss die Modalitäten zusammengeführt werden. Die Position bestimmt, ob Interaktionen bereits auf niedriger Ebene gelernt werden (früh), ob zunächst getrennte Repräsentationen entstehen (spät) oder ob die Integration mehrfach über die Modelltiefe erfolgt \cite{Jiao2024, Li2024, Baltrusaitis2019}.

\paragraph{Early Fusion (Daten- und Feature-Ebene)}
Bei der Early Fusion werden die Modalitäten auf Rohdaten- oder Feature-Ebene zu einer gemeinsamen Eingabe kombiniert und anschließend in einem gemeinsamen Modellzweig (\textit{One-Tower}) weiterverarbeitet \cite{Jiao2024, Li2024, Baltrusaitis2019}. Dadurch können Interaktionen zwischen Merkmalen früh und kontextübergreifend gelernt werden, jedoch steigen die Anforderungen an Ausrichtung und Kompatibilität der Modalitäten; bei stark heterogenen Eingaben kann dies die Modellierung erschweren \cite{Baltrusaitis2019, Jiao2024}.

\paragraph{Late Fusion (Entscheidungs- und Ausgabe-Ebene)}
Bei der Late Fusion werden Text und Tabelle zunächst in getrennten Zweigen (\textit{Two-Tower}) verarbeitet und erst am Ende über ihre High-Level-Repräsentationen oder Prädiktionen kombiniert \cite{Li2024}. Typische Operatoren sind Mittelwertbildung, Voting oder ein nachgeschaltetes MLP \cite{Baltrusaitis2019}. Der Ansatz ist modular und robust gegenüber fehlenden Modalitäten, modelliert jedoch kaum niedrigstufige Interaktionen; zudem wirken sich starke Störungen einer Modalität direkt auf die Endentscheidung aus \cite{Baltrusaitis2019}.

\paragraph{Hybrid und Deep Fusion}
Hybride Ansätze kombinieren frühe und späte Integrationspunkte, etwa indem getrennte Zweige durch zusätzliche Fusionsmodule gekoppelt werden \cite{Jiao2024, Baltrusaitis2019, Li2024}. \textit{Deep Fusion} erweitert dieses Prinzip, indem Fusion wiederholt in Zwischenschichten stattfindet, wodurch Interaktionen schichtweise verfeinert werden können \cite{Jiao2024}. Transformer-basierte Modelle realisieren dies häufig über Cross-Attention, die Modalitäten iterativ über mehrere Layer hinweg verbindet \cite{Li2024}.
